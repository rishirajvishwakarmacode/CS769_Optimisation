\documentclass[10pt, a4paper]{book}

% --- Packages ---
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[T1]{fontenc} % Font encoding
\usepackage{amsmath, amssymb, amsfonts} % AMS math packages for equations, symbols, etc.
\usepackage{amsthm} % For theorem environments
\usepackage{mathtools} % Enhancements to amsmath
\usepackage{geometry} % Custom page layout
\usepackage{graphicx} % Graphics and images (for potential diagrams)
\usepackage{hyperref} % Hyperlinks (for table of contents)
\usepackage{cleveref} % Smart referencing
\usepackage{enumitem} % Customizing lists

% --- Geometry Settings ---
\geometry{
    a4paper,
    margin=1in,
    headheight=13.6pt
}

% --- Custom Math Commands ---
\newcommand{\R}{\mathbb{R}} % Real numbers
\newcommand{\N}{\mathbb{N}} % Natural numbers
\newcommand{\Z}{\mathbb{Z}} % Integers
\DeclareMathOperator*{\argmin}{arg\,min} % argmin operator
\DeclareMathOperator*{\argmax}{arg\,max} % argmax operator
\newcommand{\grad}{\nabla} % Gradient operator
\newcommand{\Hessian}{\mathbf{H}} % Hessian matrix
\newcommand{\T}{\intercal} % Transpose symbol

% --- Theorem Environments (for formal math) ---
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}

% --- Document Metadata ---
\title{Optimization Theory: Concise Lecture Notes}
\author{Based on Class Transcripts (Generated by AI)}
\date{\today}

\begin{document}

\frontmatter % Roman numerals for front matter pages
\maketitle
\tableofcontents

\mainmatter % Arabic numerals for main content pages

% --- Start of Content ---
% The first chapter will be inserted here when you provide the transcript.
\chapter{Introduction and Overview of Mathematical Optimization}
\label{chap:intro}

This chapter provides a rapid overview of mathematical optimization, its fundamental structure, core problem classes (Least Squares, Linear Programming), and introduces the central focus of the course: Convex Optimization.

---

\section{The General Optimization Problem}

\begin{definition}[General Optimization Problem]
A mathematical optimization problem is typically expressed in the form:
$$
\begin{array}{ll}
\text{minimize} & f_0(\mathbf{x}) \\
\text{subject to} & f_i(\mathbf{x}) \le 0, \quad i = 1, \dots, m \\
& h_j(\mathbf{x}) = 0, \quad j = 1, \dots, p
\end{array}
$$
\end{definition}

\begin{itemize}[leftmargin=*, noitemsep]
    \item \textbf{Optimization Variable ($\mathbf{x}$):} The vector of variables we choose, often called \textbf{decision variables} in management science. $\mathbf{x} \in \R^n$.
    \item \textbf{Objective Function ($f_0(\mathbf{x})$):} The function to be minimized (or maximized). This is a "best effort" goal; smaller/more negative values are preferred.
    \item \textbf{Constraints ($f_i, h_j$):} These are predicates that evaluate to true or false. If an $\mathbf{x}$ violates any constraint (even slightly), it is deemed \textbf{completely unacceptable} (hard constraints).
\end{itemize}

\begin{definition}[Optimal Point]
An optimal point $\mathbf{x}^\star$ (indicated by a star, $\star$) is a choice that satisfies all constraints and achieves the smallest objective value among all constraint-satisfying choices (feasible points).
\end{definition}

\begin{remark}[Min vs. Minimize]
The term $\min$ is a mathematical operator that takes a finite set of numbers and returns the smallest one. $\mathbf{Minimize}$ is a problem constructor indicating the objective of the optimization problem.
\end{remark}

---

\section{Core Problem Classes}
The parent class for the most important problem types is Convex Optimization.

\subsection{Least Squares (LS) Problem}

LS is arguably the most important problem class. It minimizes the $\ell_2$-norm squared of the residual.

\begin{definition}[Least Squares Problem]
$$
\text{minimize}_{\mathbf{x} \in \R^n} \quad \| \mathbf{A}\mathbf{x} - \mathbf{b} \|_2^2 = \sum_{i=1}^m (\mathbf{a}_i^\T \mathbf{x} - b_i)^2
$$
\end{definition}

\begin{itemize}[leftmargin=*, noitemsep]
    \item \textbf{History:} Dates back to approximately 1800 (Gauss and Legendre).
    \item \textbf{Status:} A \textbf{mature technology}. Solvable efficiently even for gigantic problems.
\end{itemize}

\begin{theorem}[Analytical Solution for LS]
If $\mathbf{A} \in \R^{m \times n}$ has full column rank and $m \ge n$ (tall $\mathbf{A}$), the unique optimal solution $\mathbf{x}^\star$ is given by:
$$
\mathbf{x}^\star = (\mathbf{A}^\T \mathbf{A})^{-1} \mathbf{A}^\T \mathbf{b}
$$
This is found by setting the gradient of the objective to zero, solving the normal equations $\mathbf{A}^\T \mathbf{A}\mathbf{x} = \mathbf{A}^\T \mathbf{b}$.
\end{theorem}

\begin{proposition}[Computational Complexity]
Least squares problems can be solved in time proportional to $m n^2$ (the "big time small squared theme"), where $m$ is the number of rows/terms and $n$ is the dimension of $\mathbf{x}$.
\end{proposition}

\subsection{Linear Programming (LP)}
LP is characterized by linear objectives and linear constraints.

\begin{definition}[Linear Program (Standard Form)]
$$
\begin{array}{ll}
\text{minimize}_{\mathbf{x} \in \R^n} & \mathbf{c}^\T \mathbf{x} \\
\text{subject to} & \mathbf{A}\mathbf{x} = \mathbf{b} \\
& \mathbf{x} \ge \mathbf{0}
\end{array}
$$
A more general form is $\min_{\mathbf{x}} \mathbf{c}^\T \mathbf{x} \quad \text{s.t.} \quad \mathbf{G}\mathbf{x} \le \mathbf{h}$.
\end{definition}


\begin{itemize}[leftmargin=*, noitemsep]
    \item \textbf{Feasible Set:} Defined by linear inequalities (half-spaces) and equalities. This region is a \textbf{polyhedron}.
    \item \textbf{Solution Property:} The optimum is often found at a \textbf{vertex} (corner point) of the polyhedron.
    \item \textbf{Status:} LP is a non-problem, efficiently solvable even with $10^6$ variables and $10^7$ constraints. It is widely used in supply chain, scheduling, and structural design.
    \item \textbf{Complexity Paradox:} A problem with $n=1000$ variables and $m=5000$ inequalities has $\approx \binom{5000}{1000}$ vertices, yet it is solvable quickly (e.g., 50 milliseconds on a laptop).
\end{itemize}

\subsection{Convex Optimization}
The underlying structure uniting LS and LP is convexity.

\begin{definition}[Convex Function]
A function $f: \R^n \to \R$ is \textbf{convex} if for any $\mathbf{x}, \mathbf{y} \in \R^n$ and any $\alpha \in [0, 1]$ (where $\beta = 1 - \alpha$), the following inequality holds:
$$
f(\alpha \mathbf{x} + \beta \mathbf{y}) \le \alpha f(\mathbf{x}) + \beta f(\mathbf{y})
$$
\end{definition}


\begin{itemize}[leftmargin=*, noitemsep]
    \item \textbf{Geometric Interpretation:} The function lies below the line segment connecting any two points on its graph. In 1D, this means the function "curves up" (\textbf{non-negative curvature}).
    \item \textbf{The Asymmetry:} The entire field focuses on functions with non-negative curvature. \textbf{Non-negative curvature is fine; negative curvature makes problems hard} (non-convex).
\end{itemize}

\begin{definition}[Convex Optimization Problem]
A general optimization problem is convex if:
\begin{enumerate}[label=(\roman*)]
    \item The objective function $f_0(\mathbf{x})$ is convex.
    \item The inequality constraint functions $f_i(\mathbf{x})$ are convex.
    \item The equality constraint functions $h_j(\mathbf{x})$ are affine (linear plus a constant: $h_j(\mathbf{x}) = \mathbf{a}_j^\T \mathbf{x} + b_j$).
\end{enumerate}
\end{definition}

\begin{theorem}[Global Optimality]
For convex optimization problems, any solution found is guaranteed to be the \textbf{global optimum} ($\mathbf{x}^\star$).
\end{theorem}

---

\section{Applications and Solution Status}

Optimization applications are broadly divided by the nature of the variables:

\begin{enumerate}[leftmargin=*, noitemsep]
    \item \textbf{Prescriptive:} Variables are \textbf{actions} that cause things to happen (e.g., portfolio trade lists, drone thrusts, device sizes).
    \item \textbf{Descriptive:} Variables are \textbf{parameters} in a model (e.g., coefficients in a regression model or parameters in a neural network).
\end{enumerate}

\subsection{The Challenge of Solvability}
In general, optimization problems \textbf{cannot be solved} globally. This leads to two common compromises:

\begin{itemize}[leftmargin=*, noitemsep]
    \item \textbf{Local Optimization (The Asterisk Compromise):} Finds a feasible point that is better than the starting point, but offers no guarantee of being the absolute best ($\mathbf{x}^\star$).
        \begin{itemize}
            \item \textbf{Example:} Training neural networks (e.g., using Stochastic Gradient Descent). The objective (loss function) is a surrogate; the real goal is performance on unseen data.
        \end{itemize}
    \item \textbf{Global Optimization (Convex Solvers):} For LS, LP, and Convex problems, the solution is exact (no asterisk). This reliability is crucial for embedded systems (e.g., jet engine control, running 50 times per second).
\end{itemize}

\subsection{Example: The Illumination Problem}

\begin{example}[Illumination Design]
Choose lamp powers $\mathbf{p} \in \R^n$ (where $0 \le p_j \le p_{\max}$) to achieve uniform illumination $\mathbf{I}_{\text{desired}}$ across $m$ surface patches. The illumination $I_k$ on patch $k$ is linear in the lamp powers: $I_k = \sum_j a_{kj} p_j$.
\end{example}

The goal is to minimize the maximum fractional error, typically by minimizing a log-ratio objective:
$$
\text{minimize}_{\mathbf{p}} \quad \max_k \left| \log \left( \frac{I_k(\mathbf{p})}{I_{\text{desired}}} \right) \right| \quad \text{s.t.} \quad \mathbf{0} \le \mathbf{p} \le \mathbf{p}_{\max}
$$
\begin{itemize}[leftmargin=*, noitemsep]
    \item \textbf{Convexity:} This problem is entirely \textbf{convex}. The function $f(u) = \max\{u, 1/u\}$ for $u = I_k/I_{\text{desired}}$ is convex (it curves upward).
    \item \textbf{Complexity Non-Continuity:} Two constraints that sound intuitively similar can have vastly different complexity:
        \begin{enumerate}
            \item \textbf{Convex:} No more than half the total power is in any 10 lamps. (Convex).
            \item \textbf{Non-Convex/Hard:} No more than half the lamps are on (sparse solution requirement, e.g., $\text{count}(\mathbf{p}_j > 0) \le n/2$). (Not convex/Hard).
        \end{enumerate}
    The key goal of this course is to develop the mathematical intuition to distinguish between easy (convex) and hard (non-convex) problems, as this distinction is \textbf{not obvious}.
\end{itemize}
\end{document}